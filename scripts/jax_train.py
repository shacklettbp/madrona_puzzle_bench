import jax
from jax import lax, random, numpy as jnp
from jax.experimental import checkify
import flax
from flax import linen as nn

import argparse
from functools import partial
from time import time
import os

import madrona_puzzle_bench
from madrona_puzzle_bench import SimFlags, RewardMode

import madrona_learn
from madrona_learn import (
    TrainConfig, CustomMetricConfig, PPOConfig, PBTConfig,
)

from madrona_learn.rnn import LSTM
from jax_policy import make_policy

madrona_learn.init(0.7)

arg_parser = argparse.ArgumentParser()
arg_parser.add_argument('--gpu-id', type=int, default=0)
arg_parser.add_argument('--ckpt-dir', type=str, required=True)
arg_parser.add_argument('--restore', type=int)

arg_parser.add_argument('--use-fixed-world', action='store_true')
arg_parser.add_argument('--reward-mode', type=str, required=True)

arg_parser.add_argument('--num-worlds', type=int, required=True)
arg_parser.add_argument('--num-updates', type=int, required=True)
arg_parser.add_argument('--steps-per-update', type=int, default=40)
arg_parser.add_argument('--num-bptt-chunks', type=int, default=8)

arg_parser.add_argument('--lr', type=float, default=1e-4)
arg_parser.add_argument('--gamma', type=float, default=0.998)
arg_parser.add_argument('--entropy-loss-coef', type=float, default=0.01)
arg_parser.add_argument('--value-loss-coef', type=float, default=0.5)
arg_parser.add_argument('--clip-value-loss', action='store_true')

arg_parser.add_argument('--num-channels', type=int, default=256)
arg_parser.add_argument('--separate-value', action='store_true')
arg_parser.add_argument('--fp16', action='store_true')
arg_parser.add_argument('--bf16', action='store_true')

arg_parser.add_argument('--pbt-ensemble-size', type=int, default=1)
arg_parser.add_argument('--pbt-past-policies', type=int, default=0)

arg_parser.add_argument('--gpu-sim', action='store_true')
arg_parser.add_argument('--profile-port', type=int, default=None)

args = arg_parser.parse_args()

sim_flags = SimFlags.Default
if args.use_fixed_world:
    sim_flags |= SimFlags.UseFixedWorld

reward_mode = getattr(RewardMode, args.reward_mode)

sim = madrona_puzzle_bench.SimManager(
    exec_mode = madrona_puzzle_bench.madrona.ExecMode.CUDA if args.gpu_sim else madrona_puzzle_bench.madrona.ExecMode.CPU,
    gpu_id = args.gpu_id,
    num_worlds = args.num_worlds,
    auto_reset = True,
    sim_flags = sim_flags,
    reward_mode = reward_mode,
)

jax_gpu = jax.devices()[0].platform == 'gpu'

sim_step, init_sim_data = sim.jax(jax_gpu)

def metrics_cb(metrics, epoch, mb, train_state):
    return metrics

last_time = 0
last_update = 0

def host_cb(update_id, metrics, train_state_mgr):
    global last_time, last_update

    cur_time = time()
    update_diff = update_id - last_update

    print(f"Update: {update_id}")
    if last_time != 0:
        print("  FPS:", args.num_worlds * args.steps_per_update * update_diff / (cur_time - last_time))

    last_time = cur_time
    last_update = update_id

    metrics.pretty_print()
    vnorm_mu = train_state_mgr.train_states.value_normalizer_state['mu'][0][0]
    vnorm_sigma = train_state_mgr.train_states.value_normalizer_state['sigma'][0][0]
    print(f"    Value Normalizer => Mean: {vnorm_mu: .3e}, Ïƒ: {vnorm_sigma: .3e}")

    print()

    if update_id % 100 == 0:
        train_state_mgr.save(update_id, f"{args.ckpt_dir}/{update_id}")

    return ()

def iter_cb(update_idx, metrics, train_state_mgr):
    cb = partial(jax.experimental.io_callback, host_cb, ())
    noop = lambda *args: ()

    update_id = update_idx + 1

    should_callback = jnp.logical_or(update_id == 1, update_id % 10 == 0)

    lax.cond(should_callback, cb, noop,
             update_id, metrics, train_state_mgr)

    return should_callback

dev = jax.devices()[0]

if args.pbt_ensemble_size != 1 or args.pbt_past_policies != 0:
    pbt_cfg = PBTConfig(
        num_teams = 2,
        team_size = 1,
        num_train_policies = args.pbt_ensemble_size,
        num_past_policies = args.pbt_past_policies,
        past_policy_update_interval = 20,
        self_play_portion = 0.75,
        cross_play_portion = 0.0,
        past_play_portion = 0.25,
        #self_play_portion = 0.8,
        #cross_play_portion = 0.05,
        #past_play_portion = 0.15,
    )
else:
    pbt_cfg = None

if args.fp16:
    dtype = jnp.float16
elif args.bf16:
    dtype = jnp.bfloat16
else:
    dtype = jnp.float32

cfg = TrainConfig(
    num_worlds = args.num_worlds,
    num_agents_per_world = 2,
    num_updates = args.num_updates,
    steps_per_update = args.steps_per_update,
    num_bptt_chunks = args.num_bptt_chunks,
    lr = args.lr,
    gamma = args.gamma,
    gae_lambda = 0.95,
    algo = PPOConfig(
        num_mini_batches = 2,
        clip_coef = 0.2,
        value_loss_coef = args.value_loss_coef,
        entropy_coef = args.entropy_loss_coef,
        max_grad_norm = 0.5,
        num_epochs = 2,
        clip_value_loss = args.clip_value_loss,
        huber_value_loss = False,
    ),
    pbt = pbt_cfg,
    value_normalizer_decay = 0.999,
    compute_dtype = dtype,
    seed = 5,
)

policy, obs_preprocess = make_policy(dtype, True)

if args.restore:
    restore_ckpt = os.path.join(args.ckpt_dir, str(args.restore))
else:
    restore_ckpt = None

madrona_learn.train(dev, cfg, sim_step, init_sim_data, policy, obs_preprocess,
    iter_cb, CustomMetricConfig(add_metrics = lambda metrics: metrics),
    restore_ckpt = restore_ckpt, profile_port = args.profile_port)

del sim
